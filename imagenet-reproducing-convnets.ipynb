{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6799,"databundleVersionId":4225553,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/giuliobenedetti/imagenet-reproducing-convnets?scriptVersionId=156480203\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we aim to reproduce the model from [_Simonyan & Zisserman 2015_](https://arxiv.org/pdf/1409.1556v6.pdf). In other words, we will design an analogous model on TensorFlow, train it with the data from the [ImageNet Object Classification Challenge](https://www.kaggle.com/competitions/imagenet-object-localization-challenge) and ultimately benchmark its performance with the original model.","metadata":{}},{"cell_type":"code","source":"# Import packages\n\n# Data analysis\nimport numpy as np\nimport pandas as pd\n\n# File management\nimport os\nimport shutil\n\n# Image visualisation\nimport matplotlib.pyplot as plt\n\n# Neural network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\n\ntf.keras.utils.set_random_seed(123)\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:47:52.173202Z","iopub.execute_input":"2023-12-25T03:47:52.173673Z","iopub.status.idle":"2023-12-25T03:48:07.8714Z","shell.execute_reply.started":"2023-12-25T03:47:52.173645Z","shell.execute_reply":"2023-12-25T03:48:07.870514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If TPU is available\ntry:\n    # Detect TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\n\nexcept:\n    tpu = None\n    \n\n# If TPU is defined\nif tpu:\n    # Initialise TPU\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # Instantiate a TPU distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nelse:\n    # Or instanstiate available strategy\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:48:11.872206Z","iopub.execute_input":"2023-12-25T03:48:11.872768Z","iopub.status.idle":"2023-12-25T03:48:19.961616Z","shell.execute_reply.started":"2023-12-25T03:48:11.872734Z","shell.execute_reply":"2023-12-25T03:48:19.960723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we list the constants that will be used for preprocessing and model design.","metadata":{}},{"cell_type":"code","source":"# Define constants\n\n# Set batch size for mini-batch gradient descent\nBATCH_SIZE = 256\n# Set number of epochs to train model\nEPOCH_NUM = 50\n\n# Set kernel size for Conv2D layers\nKERNEL_SIZE = 3\n# Set padding mode for Conv2D layers\nPAD_MODE = \"same\"\n# Set activation function for Conv2D layers\nACTIVATION = \"relu\"\n\n# Set pool size for MaxPool2D layers\nPOOL_SIZE = 2\n# Set strides for MaxPool2D layers\nPOOL_STRIDES = 2","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:48:26.913551Z","iopub.execute_input":"2023-12-25T03:48:26.913896Z","iopub.status.idle":"2023-12-25T03:48:26.918216Z","shell.execute_reply.started":"2023-12-25T03:48:26.913868Z","shell.execute_reply":"2023-12-25T03:48:26.917455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\nBecause the train set is big (`train_dir`), we take a subset of classes defined by `CLASS_NUM` and process the images belonging to that subset of classes by rescaling and cropping them. Initially, we will train the model using only this subset.","metadata":{}},{"cell_type":"code","source":"# Store paths to base, train set and subset dirs\nbase_dir = \"/kaggle/input/imagenet-object-localization-challenge/\"\ndata_dir = base_dir + \"ILSVRC/Data/CLS-LOC/\"\n\n# Fetch train set\nraw_train_ds = tf.data.Dataset.list_files(data_dir + \"train/n01*/*.JPEG\", shuffle=False)\n\n# Find size of train set\ntrain_size = tf.data.experimental.cardinality(raw_train_ds).numpy()\n\n# Shuffle train set\nraw_train_ds = raw_train_ds.shuffle(train_size, reshuffle_each_iteration=False)\n\nprint(f\"Size of train set: {train_size}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:48:30.085509Z","iopub.execute_input":"2023-12-25T03:48:30.086316Z","iopub.status.idle":"2023-12-25T03:48:32.903028Z","shell.execute_reply.started":"2023-12-25T03:48:30.086282Z","shell.execute_reply":"2023-12-25T03:48:32.901986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import and extract devel labels\ndevel_df = pd.read_csv(base_dir + \"LOC_val_solution.csv\")\ny_devel = devel_df[\"PredictionString\"].str.split(expand=True)[0]\n\n# Select devel images belonging to subset of classes\nkeep = np.where(y_devel.str.startswith(\"n01\").to_numpy())[0]\ndevel_files = np.array(os.listdir(data_dir + \"val/\"))[keep]\n\n# Fetch devel set\ndevel_files = np.array([data_dir + \"val/\" + file for file in devel_files])\nraw_devel_ds = tf.data.Dataset.list_files(devel_files, shuffle=False)\n\n# Select labels belonging to subset of classes\ny_devel = y_devel[y_devel.str.startswith(\"n01\")].values\n\n# Find size of devel set\ndevel_size = tf.data.experimental.cardinality(raw_devel_ds).numpy()\n\nprint(f\"Size of train set: {devel_size}\")\nprint(\"Examples of labels:\")\ny_devel[:10]","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:48:44.095296Z","iopub.execute_input":"2023-12-25T03:48:44.09575Z","iopub.status.idle":"2023-12-25T03:49:01.116332Z","shell.execute_reply.started":"2023-12-25T03:48:44.095715Z","shell.execute_reply":"2023-12-25T03:49:01.115457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find class names from dir names\nclass_names = np.array(sorted([dir for dir in os.listdir(data_dir + \"train/\") if dir.startswith(\"n01\")]))\n\n# Set number of classes\nCLASS_NUM = len(class_names)\n\nfor f in raw_train_ds.take(5):\n    print(f.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:17.154322Z","iopub.execute_input":"2023-12-25T03:49:17.154668Z","iopub.status.idle":"2023-12-25T03:49:17.333719Z","shell.execute_reply.started":"2023-12-25T03:49:17.154639Z","shell.execute_reply":"2023-12-25T03:49:17.332843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define label extractor for train samples\ndef extract_train_label(file_path):\n    \n    # Split file path into parts\n    parts = tf.strings.split(file_path, os.path.sep)\n    # Extract class dir name\n    one_hot = parts[-2] == class_names\n    # Find index of maximum\n    label = tf.argmax(one_hot)\n    \n    return label\n\n# Define label extractor for devel samples\ndef extract_devel_label(file_path):\n    \n    # One-hot encode indices\n    idx_hot = devel_files == file_path\n    # Find index\n    idx = tf.argmax(idx_hot)\n    # Extract class name\n    class_name = tf.gather(y_devel, idx)\n\n    # Extract class dir name\n    one_hot = class_name == class_names\n    # Find index of maximum\n    label = tf.argmax(one_hot)\n    \n    return label\n\n# Define sample preprocessing pipeline\ndef process_path(file_path, label_fun=extract_train_label, cut=224, S=256, max_delta=0.2):\n    \n    # Convert the compressed string to a 3D uint8 tensor\n    image = tf.io.read_file(file_path)\n    image = tf.io.decode_jpeg(image, channels=3)\n    \n    # Find rescaling factor\n    min_side = tf.math.minimum(tf.shape(image)[0],tf.shape(image)[1])\n    scale = S / min_side\n   \n    #Zero-mean the RGB values by ImageNet Mean\n    image = tf.keras.applications.vgg16.preprocess_input(image)\n\n    # Compute new dimensions\n    new_height = tf.cast(tf.shape(image)[0], tf.float64) * scale\n    new_width = tf.cast(tf.shape(image)[1], tf.float64) * scale\n    \n    # Convert to float\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # Rescale by S\n    image = tf.image.resize(image, size=(new_height, new_width), preserve_aspect_ratio=True)\n    # Crop randomly\n    image = tf.image.random_crop(image, size=[cut, cut, 3], name=None)\n    # adjust RGB values by random amount\n    image = tf.image.random_hue(image, max_delta=max_delta)\n    # randomly flip over horizontal axis\n    image = tf.image.random_flip_left_right(image)\n    \n    label = label_fun(file_path)\n    \n    return image, label\n\nimage, label = process_path(data_dir + \"val/ILSVRC2012_val_00046886.JPEG\", label_fun=extract_devel_label)\ninput_shape = image.shape\n\nplt.imshow(image)\nprint(label)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:19.659515Z","iopub.execute_input":"2023-12-25T03:49:19.660212Z","iopub.status.idle":"2023-12-25T03:49:20.015219Z","shell.execute_reply.started":"2023-12-25T03:49:19.660177Z","shell.execute_reply":"2023-12-25T03:49:20.014483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess images\ntrain_ds = raw_train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\ndevel_ds = raw_devel_ds.map(lambda path: process_path(path, label_fun=extract_devel_label, max_delta=0), num_parallel_calls=tf.data.AUTOTUNE)\n\n# Show example\nfor image, label in train_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:25.994263Z","iopub.execute_input":"2023-12-25T03:49:25.994609Z","iopub.status.idle":"2023-12-25T03:49:26.639833Z","shell.execute_reply.started":"2023-12-25T03:49:25.99458Z","shell.execute_reply":"2023-12-25T03:49:26.638845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Improve train set performance\ntrain_ds = train_ds \\\n    .cache() \\\n    .shuffle(buffer_size=1000) \\\n    .batch(BATCH_SIZE) \\\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Improve devel set performance\ndevel_ds = devel_ds \\\n    .batch(BATCH_SIZE) \\\n    .cache() \\\n    .prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:29.096841Z","iopub.execute_input":"2023-12-25T03:49:29.097209Z","iopub.status.idle":"2023-12-25T03:49:29.112539Z","shell.execute_reply.started":"2023-12-25T03:49:29.097176Z","shell.execute_reply":"2023-12-25T03:49:29.111784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train_ds))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i])\n    label = label_batch[i]\n    plt.title(class_names[label])\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:32.311859Z","iopub.execute_input":"2023-12-25T03:49:32.312199Z","iopub.status.idle":"2023-12-25T03:49:34.309993Z","shell.execute_reply.started":"2023-12-25T03:49:32.31217Z","shell.execute_reply":"2023-12-25T03:49:34.309166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Design","metadata":{}},{"cell_type":"markdown","source":"In general, the model architecture resembles the model from the paper pretty well, even though some details may still be missing.","metadata":{}},{"cell_type":"code","source":"# Build model\ndef design_model():\n    \n    model = keras.models.Sequential([\n\n        # 1st convolutional block\n        layers.Conv2D(input_shape=input_shape, filters=64, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 2nd convolutional block\n        layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 3rd convolutional block\n        layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 4th convolutional block\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 5th convolutional block\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, kernel_initializer=initializers.RandomNormal(stddev=0.01), padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # Classifier head\n        layers.Flatten(),\n        layers.Dense(4096, activation=ACTIVATION, kernel_initializer=initializers.RandomNormal(stddev=0.01)),\n        layers.Dropout(rate=0.5),\n        layers.Dense(4096, activation=ACTIVATION, kernel_initializer=initializers.RandomNormal(stddev=0.01)),\n        layers.Dropout(rate=0.5),\n        layers.Dense(CLASS_NUM, activation=ACTIVATION, kernel_initializer=initializers.RandomNormal(stddev=0.01)),\n        layers.Softmax()\n    ])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:39.183264Z","iopub.execute_input":"2023-12-25T03:49:39.183602Z","iopub.status.idle":"2023-12-25T03:49:39.191909Z","shell.execute_reply.started":"2023-12-25T03:49:39.183574Z","shell.execute_reply":"2023-12-25T03:49:39.190931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimiser():\n    \n    sgd_optimiser = keras.optimizers.experimental.SGD(\n            learning_rate=1e-2,\n            momentum=0.9,\n            nesterov=False,\n            weight_decay=5e-4\n        )\n    \n    return sgd_optimiser","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:42.606456Z","iopub.execute_input":"2023-12-25T03:49:42.606795Z","iopub.status.idle":"2023-12-25T03:49:42.61117Z","shell.execute_reply.started":"2023-12-25T03:49:42.606765Z","shell.execute_reply":"2023-12-25T03:49:42.610418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up model within scope of tpu strategy\nwith strategy.scope():\n    \n    # Build model\n    model = design_model()\n    \n    # Define optimiser\n    sgd_optimiser = create_optimiser()\n    \n    # Compile model\n    model.compile(\n        optimizer = sgd_optimiser,\n        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=[\"categorical_accuracy\"]\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:49:50.490966Z","iopub.execute_input":"2023-12-25T03:49:50.491302Z","iopub.status.idle":"2023-12-25T03:49:56.444928Z","shell.execute_reply.started":"2023-12-25T03:49:50.491273Z","shell.execute_reply":"2023-12-25T03:49:56.444099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To test that the model works, we train it for a few epochs. Ideally, we will train the model on the full dataset when once it resembles the model from the paper in terms of parameter number. ","metadata":{}},{"cell_type":"code","source":"# Update learning rate\nLR_Decay = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=2,\n    mode=\"min\",\n    min_delta=1e-4,\n    min_lr=1e-5\n)\n\n# Train the model\nhistory = model.fit(\n    train_ds,\n    validation_data=devel_ds,\n    epochs=EPOCH_NUM,\n    batch_size=BATCH_SIZE,\n    verbose=False,\n    callbacks=[LR_Decay]\n)\n\n# Store training history as a dataframe\nhistory_df = pd.DataFrame(history.history)\n\nprint(f\"Train loss: {history_df['loss'].iloc[-1]:.3f}, Devel loss: {history_df['val_loss'].iloc[-1]:.3f}\")\nprint(f\"Train accuracy: {history_df['categorical_accuracy'].iloc[-1]:.3f}, Devel accuracy: {history_df['val_categorical_accuracy'].iloc[-1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:51:10.114875Z","iopub.execute_input":"2023-12-25T03:51:10.115909Z","iopub.status.idle":"2023-12-25T03:54:31.295946Z","shell.execute_reply.started":"2023-12-25T03:51:10.115859Z","shell.execute_reply":"2023-12-25T03:54:31.294613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model contains 128778627 trainable parameters, which aligns pretty well with the 33 million in the paper. Our model may still lack some element responsible for the missing 4 million parameters.","metadata":{}},{"cell_type":"code","source":"# Visualise loss\nhistory_df.loc[:, [\"loss\", \"val_loss\"]].plot(title=\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:54:35.863064Z","iopub.execute_input":"2023-12-25T03:54:35.863527Z","iopub.status.idle":"2023-12-25T03:54:36.248709Z","shell.execute_reply.started":"2023-12-25T03:54:35.863487Z","shell.execute_reply":"2023-12-25T03:54:36.24753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise accuracy\nhistory_df.loc[:, [\"sparse_categorical_accuracy\", \"val_categorical_accuracy\"]].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:54:39.568879Z","iopub.execute_input":"2023-12-25T03:54:39.569278Z","iopub.status.idle":"2023-12-25T03:54:39.776487Z","shell.execute_reply.started":"2023-12-25T03:54:39.569246Z","shell.execute_reply":"2023-12-25T03:54:39.775483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning\n\nThe next step involves building a deeper ConvNet by:\n1. taking the pretrained base from the previous section,\n2. enriching it with new untrained convolutional layers and\n3. attaching it to a new untrained classification head\n\nBy transferring the learnt weight from our previous model, we can build more complex and deeper ConvNets without too much computational burden. In particular, here we aim to reproduce the following four architectures from _Zimonyan & Zisserman 2015_:\n- 13-layer ConvNet\n- 16-layer ConvNet (with 1-by-1 filters)\n- 16-layer ConvNet (with 3-by-3 filters)\n- 19-layer ConvNet\n\nIn the following cell, we pretend that the previously trained model is stored in a keras file to show how pretrained models can be imported into a notebook.","metadata":{}},{"cell_type":"code","source":"# Define model file\nmodel_file = f\"/kaggle/working/{CLASS_NUM}class_model.keras\"\n\n# Save model into file for replication purposes\nmodel.save(model_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:54:56.258036Z","iopub.execute_input":"2023-12-25T03:54:56.25844Z","iopub.status.idle":"2023-12-25T03:55:01.251017Z","shell.execute_reply.started":"2023-12-25T03:54:56.258406Z","shell.execute_reply":"2023-12-25T03:55:01.249696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we want to add new convolutional layers between the pretrained ones, we have to separate the latter from the whole pretrained base as done below.","metadata":{}},{"cell_type":"markdown","source":"## 13-layer ConvNet\n\nHere, we build and train a 13-layer ConvNet with a half pretrained half untrained base.","metadata":{}},{"cell_type":"code","source":"# Build model\ndef design_model(model_file, layer_positions=[0, 2, 4, 5, 7, 8, 10, 11]):\n    \n    # Import pretrained base\n    pretrained_base = keras.models.load_model(model_file)\n    \n    # Select relevant layers\n    pretrained_layers = [pretrained_base.get_layer(index=i) for i in layer_positions]\n    \n    model = keras.models.Sequential([\n\n        # 1st convolutional block\n        pretrained_layers[0],\n        layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 2nd convolutional block\n        pretrained_layers[1],\n        layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 3rd convolutional block\n        pretrained_layers[2],\n        pretrained_layers[3],\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 4th convolutional block\n        pretrained_layers[4],\n        pretrained_layers[5],\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 5th convolutional block\n        pretrained_layers[6],\n        pretrained_layers[7],\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # Classifier head\n        layers.Flatten(),\n        layers.Dense(4096, activation=ACTIVATION),\n        layers.Dropout(rate=0.5),\n        layers.Dense(4096, activation=ACTIVATION),\n        layers.Dropout(rate=0.5),\n        layers.Dense(CLASS_NUM)\n    ])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:55:05.393037Z","iopub.execute_input":"2023-12-25T03:55:05.393449Z","iopub.status.idle":"2023-12-25T03:55:05.402312Z","shell.execute_reply.started":"2023-12-25T03:55:05.393409Z","shell.execute_reply":"2023-12-25T03:55:05.401399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up model within scope of tpu strategy\nwith strategy.scope():\n    \n    # Build model\n    model = design_model(model_file)\n    \n    # Define optimiser\n    sgd_optimiser = create_optimiser()\n    \n    # Compile model\n    model.compile(\n        optimizer = sgd_optimiser,\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"sparse_categorical_accuracy\"]\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:55:13.989873Z","iopub.execute_input":"2023-12-25T03:55:13.990413Z","iopub.status.idle":"2023-12-25T03:57:49.740819Z","shell.execute_reply.started":"2023-12-25T03:55:13.99036Z","shell.execute_reply":"2023-12-25T03:57:49.739509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update learning rate\nLR_Decay = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.1,\n    patience=2,\n    mode=\"min\",\n    min_delta=1e-4,\n    min_lr=1e-5\n)\n\n# Train the model\nhistory = model.fit(\n    train_ds,\n    validation_data=devel_ds,\n    epochs=70 - EPOCH_NUM,\n    batch_size=BATCH_SIZE,\n    verbose=False,\n    callbacks=[LR_Decay]\n)\n\n# Store training history as a dataframe\nhistory_df = pd.DataFrame(history.history)\n\nprint(f\"Train loss: {history_df['loss'].iloc[-1]:.3f}, Devel loss: {history_df['val_loss'].iloc[-1]:.3f}\")\nprint(f\"Train accuracy: {history_df['sparse_categorical_accuracy'].iloc[-1]:.3f}, Devel accuracy: {history_df['val_sparse_categorical_accuracy'].iloc[-1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T03:59:21.134472Z","iopub.execute_input":"2023-12-25T03:59:21.134842Z","iopub.status.idle":"2023-12-25T04:04:49.058635Z","shell.execute_reply.started":"2023-12-25T03:59:21.134811Z","shell.execute_reply":"2023-12-25T04:04:49.057393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise loss\nhistory_df.loc[:, [\"loss\", \"val_loss\"]].plot(title=\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T04:04:49.319286Z","iopub.execute_input":"2023-12-25T04:04:49.319563Z","iopub.status.idle":"2023-12-25T04:04:49.55677Z","shell.execute_reply.started":"2023-12-25T04:04:49.319523Z","shell.execute_reply":"2023-12-25T04:04:49.555777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise accuracy\nhistory_df.loc[:, [\"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\"]].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T04:04:53.533345Z","iopub.execute_input":"2023-12-25T04:04:53.534291Z","iopub.status.idle":"2023-12-25T04:04:53.745395Z","shell.execute_reply.started":"2023-12-25T04:04:53.534256Z","shell.execute_reply":"2023-12-25T04:04:53.744458Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
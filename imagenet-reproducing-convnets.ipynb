{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6799,"databundleVersionId":4225553,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/giuliobenedetti/imagenet-reproducing-convnets?scriptVersionId=155859687\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we aim to reproduce the model from [_Zimonyan & Zisserman 2015_](https://arxiv.org/pdf/1409.1556v6.pdf). In other words, we will design an analogous model on TensorFlow, train it with the data from the [ImageNet Object Classification Challenge](https://www.kaggle.com/competitions/imagenet-object-localization-challenge) and ultimately benchmark its performance with the original model.","metadata":{}},{"cell_type":"code","source":"# Import packages\n\n# Data analysis\nimport numpy as np\nimport pandas as pd\n\n# File management\nimport os\nimport shutil\n\n# Image processing\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport skimage.transform as sktr\n\n# Neural network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:02:50.309346Z","iopub.execute_input":"2023-12-20T19:02:50.310176Z","iopub.status.idle":"2023-12-20T19:03:08.658518Z","shell.execute_reply.started":"2023-12-20T19:02:50.310099Z","shell.execute_reply":"2023-12-20T19:03:08.65723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we list the constants that will be used for preprocessing and model design.","metadata":{}},{"cell_type":"code","source":"# Define constants\n\n# Set batch size for mini-batch gradient descent\nBATCH_SIZE = 256\n# Set number of epochs to train model\nEPOCH_NUM = 74\n\n# Set kernel size for Conv2D layers\nKERNEL_SIZE = 3\n# Set padding mode for Conv2D layers\nPAD_MODE = \"same\"\n# Set activation function for Conv2D layers\nACTIVATION = \"relu\"\n\n# Set pool size for MaxPool2D layers\nPOOL_SIZE = 2\n# Set strides for MaxPool2D layers\nPOOL_STRIDES = 2","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:03:18.582662Z","iopub.execute_input":"2023-12-20T19:03:18.58342Z","iopub.status.idle":"2023-12-20T19:03:18.58869Z","shell.execute_reply.started":"2023-12-20T19:03:18.583383Z","shell.execute_reply":"2023-12-20T19:03:18.58787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\nBecause the train set is big (`train_dir`), we take a subset of classes defined by `CLASS_NUM` and process the images belonging to that subset of classes by rescaling and cropping them. Initially, we will train the model using only this subset.","metadata":{}},{"cell_type":"code","source":"# Store paths to base, train set and subset dirs\nbase_dir = \"/kaggle/input/imagenet-object-localization-challenge/\"\ntrain_dir = base_dir + \"ILSVRC/Data/CLS-LOC/train/\"\ndevel_dir = base_dir + \"ILSVRC/Data/CLS-LOC/val/\"\n\ndataset = tf.data.Dataset.list_files(train_dir + \"*/*.JPEG\", shuffle=False)\nimage_count = tf.data.experimental.cardinality(dataset).numpy()\n\ndataset = dataset.shuffle(image_count, reshuffle_each_iteration=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:03:24.025784Z","iopub.execute_input":"2023-12-20T19:03:24.026256Z","iopub.status.idle":"2023-12-20T19:03:24.186739Z","shell.execute_reply.started":"2023-12-20T19:03:24.02622Z","shell.execute_reply":"2023-12-20T19:03:24.185511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = np.array(sorted(os.listdir(train_dir)))\n\n# Set number of classes to use (this is a way to subset the data)\nCLASS_NUM = len(class_names)\n\nfor f in dataset.take(5):\n    print(f.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-20T19:03:26.684215Z","iopub.execute_input":"2023-12-20T19:03:26.68461Z","iopub.status.idle":"2023-12-20T19:03:26.764346Z","shell.execute_reply.started":"2023-12-20T19:03:26.68458Z","shell.execute_reply":"2023-12-20T19:03:26.763409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_path(file_path):\n# Convert the compressed string to a 3D uint8 tensor\n    img = tf.io.read_file(file_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize_with_crop_or_pad(img, 228, 228)\n\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    label = parts[-2] == class_names\n    \n    return img, label\n\nimg, label = process_path(train_dir + \"n01440764/n01440764_10026.JPEG\")\ninput_shape = img.shape\n\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:34.953214Z","iopub.execute_input":"2023-12-20T18:53:34.953573Z","iopub.status.idle":"2023-12-20T18:53:35.452604Z","shell.execute_reply.started":"2023-12-20T18:53:34.953542Z","shell.execute_reply":"2023-12-20T18:53:35.451678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\n# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\ndataset = dataset.map(process_path, num_parallel_calls=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:35.45384Z","iopub.execute_input":"2023-12-20T18:53:35.454134Z","iopub.status.idle":"2023-12-20T18:53:35.70415Z","shell.execute_reply.started":"2023-12-20T18:53:35.454108Z","shell.execute_reply":"2023-12-20T18:53:35.70322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, label in dataset.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:35.705267Z","iopub.execute_input":"2023-12-20T18:53:35.705528Z","iopub.status.idle":"2023-12-20T18:53:35.831023Z","shell.execute_reply.started":"2023-12-20T18:53:35.705505Z","shell.execute_reply":"2023-12-20T18:53:35.830107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ndataset = configure_for_performance(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:35.832501Z","iopub.execute_input":"2023-12-20T18:53:35.8328Z","iopub.status.idle":"2023-12-20T18:53:35.849375Z","shell.execute_reply.started":"2023-12-20T18:53:35.832754Z","shell.execute_reply":"2023-12-20T18:53:35.848363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(dataset))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n    label = label_batch[i]\n    plt.title(class_names[label])\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:35.8528Z","iopub.execute_input":"2023-12-20T18:53:35.853098Z","iopub.status.idle":"2023-12-20T18:53:38.298571Z","shell.execute_reply.started":"2023-12-20T18:53:35.853072Z","shell.execute_reply":"2023-12-20T18:53:38.297436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Design","metadata":{}},{"cell_type":"markdown","source":"In general, the model architecture resembles the model from the paper pretty well, even though some details may still be missing.","metadata":{}},{"cell_type":"code","source":"# Design model\ndef create_model():\n    \n    model = keras.models.Sequential([\n\n        # 1st convolutional block\n        layers.Conv2D(input_shape=input_shape, filters=64, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 2nd convolutional block\n        layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 3rd convolutional block\n        layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=256, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 4th convolutional block\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # 5th convolutional block\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.Conv2D(filters=512, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n        layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n\n        # Classifier head\n        layers.Flatten(),\n        layers.Dense(4096, activation=ACTIVATION),\n        layers.Dropout(rate=0.5),\n        layers.Dense(4096, activation=ACTIVATION),\n        layers.Dropout(rate=0.5),\n        layers.Dense(CLASS_NUM, activation=\"softmax\")\n    ])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:38.29976Z","iopub.execute_input":"2023-12-20T18:53:38.300064Z","iopub.status.idle":"2023-12-20T18:53:38.623934Z","shell.execute_reply.started":"2023-12-20T18:53:38.300039Z","shell.execute_reply":"2023-12-20T18:53:38.623094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    \n    model = create_model()\n    \n    # Define optimiser\n    sgd_optimiser = keras.optimizers.experimental.SGD(\n        learning_rate=0.01,\n        momentum=0.90,\n        nesterov=False,\n        weight_decay=0.0005\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer = sgd_optimiser,\n        loss=\"categorical_crossentropy\",\n        metrics=[\"categorical_accuracy\"]\n    )\n\nsteps_per_epoch = 60000 // BATCH_SIZE\n\nmodel.fit(train_dataset,\n          epochs=EPOCH_NUM,\n          steps_per_epoch=steps_per_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:38.625037Z","iopub.execute_input":"2023-12-20T18:53:38.625337Z","iopub.status.idle":"2023-12-20T18:53:38.646231Z","shell.execute_reply.started":"2023-12-20T18:53:38.62531Z","shell.execute_reply":"2023-12-20T18:53:38.645366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To test that the model works, we train it for a few epochs. Ideally, we will train the model on the full dataset when once it resembles the model from the paper in terms of parameter number. ","metadata":{}},{"cell_type":"code","source":"# Update learning rate\nLR_Decay = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"loss\",\n    factor=0.1,\n    patience=2,\n    mode=\"min\",\n    min_delta=1e-4,\n    min_lr=1e-5\n)\n\n# Train the model\nhistory = model.fit(\n    dataset,\n    epochs=EPOCH_NUM,\n    steps_per_epoch=steps_per_epoch\n    batch_size=BATCH_SIZE,\n    verbose=True,\n    callbacks=[LR_Decay]\n)\n\n# Store training history as a dataframe\nhistory_df = pd.DataFrame(history.history)\n\nprint(f\"Train loss: {history_df['loss'].iloc[-1]:.3f}\")\nprint(f\"Train accuracy: {history_df['categorical_accuracy'].iloc[-1]:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:53:38.647796Z","iopub.execute_input":"2023-12-20T18:53:38.648146Z","iopub.status.idle":"2023-12-20T18:54:41.20325Z","shell.execute_reply.started":"2023-12-20T18:53:38.648113Z","shell.execute_reply":"2023-12-20T18:54:41.20234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model contains 128778627 trainable parameters, which aligns pretty well with the 33 million in the paper. Our model may still lack some element responsible for the missing 4 million parameters.","metadata":{}},{"cell_type":"code","source":"# Visualise loss\nhistory_df.loc[:, \"loss\"].plot(title=\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:54:41.204529Z","iopub.execute_input":"2023-12-20T18:54:41.204918Z","iopub.status.idle":"2023-12-20T18:54:41.444917Z","shell.execute_reply.started":"2023-12-20T18:54:41.204884Z","shell.execute_reply":"2023-12-20T18:54:41.444001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise accuracy\nhistory_df.loc[:, \"categorical_accuracy\"].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:54:41.446304Z","iopub.execute_input":"2023-12-20T18:54:41.446983Z","iopub.status.idle":"2023-12-20T18:54:41.751517Z","shell.execute_reply.started":"2023-12-20T18:54:41.446946Z","shell.execute_reply":"2023-12-20T18:54:41.750591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning\n\nThe next step involves building a deeper ConvNet by:\n1. taking the pretrained base from the previous section,\n2. enriching it with new untrained convolutional layers and\n3. attaching it to a new untrained classification head\n\nBy transferring the learnt weight from our previous model, we can build more complex and deeper ConvNets without too much computational burden. In particular, here we aim to reproduce the following four architectures from _Zimonyan & Zisserman 2015_:\n- 13-layer ConvNet\n- 16-layer ConvNet (with 1-by-1 filters)\n- 16-layer ConvNet (with 3-by-3 filters)\n- 19-layer ConvNet\n\nIn the following cell, we pretend that the previously trained model is stored in a keras file to show how pretrained models can be imported into a notebook.","metadata":{}},{"cell_type":"code","source":"'''# Define model file\nmodel_file = f\"/kaggle/working/{CLASS_NUM}class_model.keras\"\n\n# Save model into file for replication purposes\nmodel.save(model_file)\n\n# Import pretrained base\nnew_model = keras.models.load_model(model_file)\n\n# IMPORTANT: Set weights from base as untrainable\nnew_model.trainable = False'''","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:54:41.752772Z","iopub.execute_input":"2023-12-20T18:54:41.753161Z","iopub.status.idle":"2023-12-20T18:55:01.972073Z","shell.execute_reply.started":"2023-12-20T18:54:41.753125Z","shell.execute_reply":"2023-12-20T18:55:01.971181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we want to add new convolutional layers between the pretrained ones, we have to separate the latter from the whole pretrained base as done below.","metadata":{}},{"cell_type":"code","source":"'''# List indices of relevant layers (only convolutional, no maxpool)\nlayer_positions = [0, 2, 4, 5, 7, 8, 10, 11]\n\n# Select relevant layers\npretrained_layers = [new_model.get_layer(index=i) for i in layer_positions]'''","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:55:01.973216Z","iopub.execute_input":"2023-12-20T18:55:01.973496Z","iopub.status.idle":"2023-12-20T18:55:01.979207Z","shell.execute_reply.started":"2023-12-20T18:55:01.973472Z","shell.execute_reply":"2023-12-20T18:55:01.978179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13-layer ConvNet\n\nHere, we build and train a 13-layer ConvNet with a half pretrained half untrained base.","metadata":{}},{"cell_type":"code","source":"'''# Design model\nmodel = keras.models.Sequential([\n    \n    # 1st convolutional block\n    pretrained_layers[0],\n    layers.Conv2D(filters=64, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n    layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n    \n    # 2nd convolutional block\n    pretrained_layers[1],\n    layers.Conv2D(filters=128, kernel_size=KERNEL_SIZE, padding=PAD_MODE, activation=ACTIVATION),\n    layers.MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n    \n    # 3rd convolutional block\n    pretrained_layers[2],\n    pretrained_layers[3],\n    layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n    \n    # 4th convolutional block\n    pretrained_layers[4],\n    pretrained_layers[5],\n    layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n    \n    # 5th convolutional block\n    pretrained_layers[6],\n    pretrained_layers[7],\n    layers.MaxPool2D(pool_size=POOL_SIZE, strides=POOL_STRIDES),\n    \n    # Classifier head\n    layers.Flatten(),\n    layers.Dense(4096, activation=ACTIVATION),\n    layers.Dropout(rate=0.5),\n    layers.Dense(4096, activation=ACTIVATION),\n    layers.Dropout(rate=0.5),\n    layers.Dense(CLASS_NUM, activation=\"softmax\")\n])\n\n# Choose optimiser, loss function and validation metric\nmodel.compile(\n    optimizer=keras.optimizers.experimental.SGD(momentum=0.9, weight_decay=0.0005),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"categorical_accuracy\"]\n)'''","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:55:01.980603Z","iopub.execute_input":"2023-12-20T18:55:01.980904Z","iopub.status.idle":"2023-12-20T18:55:02.136506Z","shell.execute_reply.started":"2023-12-20T18:55:01.980879Z","shell.execute_reply":"2023-12-20T18:55:02.135748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''# Train the model\nhistory = model.fit(\n    dataset,\n    epochs=2,\n    batch_size=BATCH_SIZE,\n    verbose=True\n)\n\n# Store training history as a dataframe\nhistory_df = pd.DataFrame(history.history)\n\nprint(f\"Train loss: {history_df['loss'].iloc[-1]:.3f}\")\nprint(f\"Train accuracy: {history_df['categorical_accuracy'].iloc[-1]:.3f}\")'''","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:55:02.137569Z","iopub.execute_input":"2023-12-20T18:55:02.137868Z","iopub.status.idle":"2023-12-20T18:55:03.007063Z","shell.execute_reply.started":"2023-12-20T18:55:02.137842Z","shell.execute_reply":"2023-12-20T18:55:03.005782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise loss\n#history_df.loc[:, \"loss\"].plot(title=\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:55:03.007997Z","iopub.status.idle":"2023-12-20T18:55:03.008351Z","shell.execute_reply.started":"2023-12-20T18:55:03.008179Z","shell.execute_reply":"2023-12-20T18:55:03.008196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise accuracy\n#history_df.loc[:, \"categorical_accuracy\"].plot(title=\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:55:03.010204Z","iopub.status.idle":"2023-12-20T18:55:03.010559Z","shell.execute_reply.started":"2023-12-20T18:55:03.010383Z","shell.execute_reply":"2023-12-20T18:55:03.0104Z"},"trusted":true},"execution_count":null,"outputs":[]}]}